{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0148309e",
   "metadata": {},
   "outputs": [],
   "source": [
    "/*\n",
    " * @Author: diasa diasa@gate.me\n",
    " * @Date: 2025-08-07 15:55:47\n",
    " * @LastEditors: diasa diasa@gate.me\n",
    " * @LastEditTime: 2025-08-08 13:42:56\n",
    " * @FilePath: /langjs_learn/demo.ipynb\n",
    " * @Description: 这是默认设置,请设置`customMade`, 打开koroFileHeader查看配置 进行设置: https://github.com/OBKoro1/koro1FileHeader/wiki/%E9%85%8D%E7%BD%AE\n",
    " */\n",
    "import { load } from 'dotenv'\n",
    "import { ChatOllama } from \"@langchain/community/chat_models/ollama\"\n",
    "import { HumanMessage } from \"@langchain/core/messages\"\n",
    "import { StringOutputParser } from \"@langchain/core/output_parsers\"\n",
    "const env = await load()\n",
    "const process = {\n",
    "  env\n",
    "} \n",
    "const chat = new ChatOllama({\n",
    "  model: \"qwen3:8b\",\n",
    "  baseUrl: \"http://localhost:11434\",\n",
    "})\n",
    "const outputParser = new StringOutputParser()\n",
    "const simpleChain = chat.pipe(outputParser)\n",
    "// 单个对话\n",
    "// await simpleChain.invoke([new HumanMessage(\"讲个笑话\")])\n",
    "// 批量\n",
    "// await simpleChain.batch([[new HumanMessage(\"讲个笑话\")], [new HumanMessage(\"who are you\")]])\n",
    "// 流式\n",
    "// const stream = await simpleChain.stream([new HumanMessage(\"讲个笑话\")])\n",
    "// for await (const chunk of stream) {\n",
    "//   console.log(chunk)\n",
    "// }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1e1eaef",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid URL: 'http://127.0.0.1:11434f/api/chat'",
     "output_type": "error",
     "traceback": [
      "Stack trace:",
      "TypeError: Invalid URL: 'http://127.0.0.1:11434f/api/chat'",
      "    at getSerialization (ext:deno_url/00_url.js:98:11)",
      "    at new URL (ext:deno_url/00_url.js:405:27)",
      "    at new Request (ext:deno_fetch/23_request.js:338:25)",
      "    at ext:deno_fetch/26_fetch.js:374:29",
      "    at new Promise (<anonymous>)",
      "    at fetch (ext:deno_fetch/26_fetch.js:370:20)",
      "    at createOllamaStream (file:///Users/diasa/Library/Caches/deno/npm/registry.npmjs.org/@langchain/community/0.3.50/dist/utils/ollama.js:9:28)",
      "    at createOllamaStream.next (<anonymous>)",
      "    at createOllamaChatStream (file:///Users/diasa/Library/Caches/deno/npm/registry.npmjs.org/@langchain/community/0.3.50/dist/utils/ollama.js:57:12)",
      "    at createOllamaChatStream.next (<anonymous>)"
     ]
    }
   ],
   "source": [
    "import { ChatOllama } from \"@langchain/community/chat_models/ollama\"\n",
    "//  fallback\n",
    "const fakeLLM = new ChatOllama({\n",
    "  model: \"qwen3:8b\",\n",
    "  baseUrl: \"http://localhost:11434f\",\n",
    "  maxRetries: 0,\n",
    "})\n",
    "await fakeLLM.invoke('你好')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18719945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage {\n",
       "  \"content\": \"<think>\\n嗯，用户问“你是谁”，我需要先确认他们想了解我的基本信息。首先，我要介绍我是通义千问，由通义实验室研发的超大规模语言模型。然后，要说明我的主要功能，比如回答问题、创作文字、逻辑推理等。同时，要提到我能够处理多种语言，但中文是主要语言。还要强调我的训练数据截止到2024年，所以可能无法提供最新的信息。此外，用户可能想知道我的应用场景，比如日常对话、学习、工作等，可以举几个例子。还要注意语气友好，保持口语化，避免使用专业术语。最后，可以邀请用户提出具体问题，看看他们需要什么帮助。这样既全面又自然，符合用户的需求。\\n</think>\\n\\n你好！我是通义千问，由通义实验室研发的超大规模语言模型。我的主要功能包括回答问题、创作文字、逻辑推理、编程、多语言理解等。我能够处理多种语言，但中文是我的主要语言。我的训练数据截止到2024年，因此可能无法提供最新的信息。如果你有任何问题或需要帮助，欢迎随时告诉我！\",\n",
       "  \"additional_kwargs\": {},\n",
       "  \"response_metadata\": {},\n",
       "  \"tool_calls\": [],\n",
       "  \"invalid_tool_calls\": []\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "const realLLM = new ChatOllama({\n",
    "    model: \"qwen3:8b\",\n",
    "    baseUrl: \"http://localhost:11434\",\n",
    "  })\n",
    "  const fallbackLLM = fakeLLM.withFallbacks({\n",
    "    fallbacks: [realLLM],\n",
    "  })\n",
    "  await fallbackLLM.invoke('你是谁')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "codemirror_mode": "typescript",
   "file_extension": ".ts",
   "mimetype": "text/x.typescript",
   "name": "typescript",
   "nbconvert_exporter": "script",
   "pygments_lexer": "typescript",
   "version": "5.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
